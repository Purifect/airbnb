{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#  Ebnable HTML/CSS \n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "___\n",
    "Enter Team Member Names here (double click to edit):\n",
    "\n",
    "- Name 1: DJ Dawkins\n",
    "- Name 2: Rick Fontenot\n",
    "- Name 3: Joe Lazarus\n",
    "- Name 4: Puri Rudick\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "________\n",
    "<a id=\"top\"></a>\n",
    "# Live Session Assignment Two\n",
    "In the following assignment you will be asked to fill in python code and derivations for a number of different problems. Please read all instructions carefully and turn in the rendered notebook (.ipynb file, remember to save it!!) or HTML of the rendered notebook before the end of class.\n",
    "\n",
    "## Contents\n",
    "* <a href=\"#Loading\">Loading the Classification Data</a>\n",
    "* <a href=\"#using_trees\">Using Decision Trees - Gini</a>\n",
    "\n",
    "**These contents will become available during the live session: **\n",
    "* <a href=\"#entropy\">Using Decision Trees - Entropy</a>\n",
    "* <a href=\"#multi\">Multi-way Splits</a>\n",
    "* <a href=\"#sklearn\">Decision Trees in Scikit-Learn</a>\n",
    "\n",
    "________________________________________________________________________________________________________\n",
    "<a id=\"Loading\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Loading the Classification Data\n",
    "Please run the following code to read in the \"digits\" dataset from sklearn's data loading module. This is identical to the first in class assignment for loading the data into matrices. `ds.data` is a matrix of feature values and `ds.target` is a column vector of the class output (in our case, the hand written digit we want to classify). Each class is a number (0 through 9) that we want to classify as one of ten hand written digits. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "ds = load_digits()\n",
    "\n",
    "# this holds the continuous feature data\n",
    "print('features shape:', ds.data.shape) # there are 1797 instances and 64 features per instance\n",
    "print('target shape:', ds.target.shape)\n",
    "print('range of target:', np.min(ds.target),np.max(ds.target))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features shape: (1797, 64)\n",
      "target shape: (1797,)\n",
      "range of target: 0 9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"using_trees\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Using Decision Trees\n",
    "In the videos, we talked about the splitting conditions for different attributes. Specifically, we discussed the number of ways in which it is possible to split a node, depending on the attribute types. To understand the possible splits, we need to understand the attributes. For the question below, you might find the description in the `ds['DESCR']` field to be useful. You can see the field using `print(ds['DESCR'])`\n",
    "\n",
    "**Question 1:** For the digits dataset, what are the type(s) of the attributes? How many attributes are there? What do they represent?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(ds['DESCR'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 1797\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "## Enter your comments here\n",
    "\n",
    "# The digits dataset contains images of hand-written digits totaling in 10 classes where each class refers to a digit of 0 to 9.\n",
    "# There are 1797 instances (images) with 64 attributes in the data set.  The attributes represent 8x8 image of integer pixels in the range of 0 to 16.\n",
    "# The photos below visualize the first 4 images in the dataset."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(12, 3))\n",
    "for ax, image, label in zip(axes, ds.images, ds.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Image: %i' % label)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqwAAACvCAYAAAA4yYy3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAL80lEQVR4nO3df4jf910H8Oerhq5us72rk9FVaXrbQOe0Z1OtKGrKLqCUccGZMNyYV5AEB2L+u/wzuSiTBMSlWKURxIKwzWa4zH+GNtgrbuAfObxWyibSJlKd2tJcurW1TubHP77f4FHS9dq+c9/3N/d4wAcu3x/Pz/v7uVfunve5z93VMAwBAIBeXTfpBQAAwHejsAIA0DWFFQCArimsAAB0TWEFAKBrCisAAF1TWAEA6No1W1ir6kJVLUx6HW9UVX2oqr5eVS9X1aNVdduk18T2msbZrarrq+oL47UPVbV30mti+03p7P50VT1SVRer6rmqOl1Vt0x6XWyfKZ3bD1TVuaraGG9nq+oDk17X1XTNFtZpVFXvSvKXST6V5OYk55L8xUQXBVv3lSQfT/Ifk14IvAGzSf4kye4ktyX5VpI/m+SCYAu+keRXMuoK70ryV0k+P9EVXWU7orBW1VJVfbWqPlNVl6rq6ar6mfHtz1TVs1X1a5sef29V/UNVfXN8/8qr8j5RVf9SVc9X1ac2f3VWVddV1dGqemp8/8NVdfMWl/rLSZ4chuH0MAyvJFlJckdV/XCbI8G0mZbZHYbh28MwnByG4StJvtPyGDCdpmh2vzz+mPvNYRheTvJAkp9teCiYIlM0t5eGYbgwjP5caWX0cfd97Y5Ef3ZEYR27O8kTSb4/yWcz+krkJzN6B388yQNV9c7xY19K8okkM0nuTfIbVbU/GZ2GT/LHST6W5JYkNyW5ddN+fjPJ/iS/kOQ9STaS/NHlO6vqiar61ddY448mefzyP4ZheCnJU+Pb2bmmYXbhSqZxdn8+yZNv6FVyrZmaua2qS0leSfKHSX7vTb3aaTEMwzW5JbmQZGH89lKSf950348lGZK8e9NtzyeZf42sk0k+M377t5N8btN9b0/y7U37+lqSD226/5Yk/5Nk1xbW/KdJjr/qtq8mWZr08bRt3zaNs/uqff5rkr2TPo627d+ugdn98SQXk/zcpI+lbfu2a2Bu35Hkk0nunfSxvJrbruwc/7np7f9KkmEYXn3bO5Okqu5OcjzJB5Ncn+RtSU6PH/eeJM9cftIwDC9X1fObcm5L8sWq+t9Nt30nybuT/NvrrPHFJDe+6rYbM7qmip1rGmYXrmRqZreq3pfky0l+axiGv9vKc7hmTc3cjnNfqqoHkzxXVT8yDMOzW33uNNlJlwS8EZ/N6ALmHxqG4aYkD2Z0jUiS/HuSH7z8wKr63oy+bXDZM0l+aRiGmU3bDcMwbGX4nkxyx6bsdyR5b3x7iq2b1OzCWzWx2a3Rb2M5m+R3h2H48wavhZ2jl4+512V0BvfW13vgtFJYr+z7klwchuGVqvqpJJuvIflCkg+PL8K+PqMfjKpN9z+Y5NPjD4Cpqh+oqsUt7veLST5YVR+pqhsy+nbCE8MwfP0tvh52jknNbqrqbeO5TZLrq+qGqqrv+iT4fxOZ3aq6NcnfJnlgGIYHG7wOdpZJze2+qvqJqvqeqroxyR9kdA3s1976S+qTwnpln0zyO1X1rYxK48OX7xiG4cmMLpT+fEZfPb2Y5Nkk/z1+yP0ZfbX1N+Pn/31GF3AnSarqyar62JV2OgzDc0k+kuTTGQ3e3Uk+2vSVca2byOyO/VNG3yq7Nclfj9/2e4TZqknN7q8nmUuyUlUvXt6avjKuZZOa25kkn0vyQkY/nP3eJL84jH7D0DWpxhfs8iaNf1LwUpL3D8NwfsLLgS0zu0wrs8s0MrdvjTOsb0JVfbiq3j6+xvT3k/xjRj9lCF0zu0wrs8s0MrftKKxvzmJGf2XiG0nen+Sjg1PVTAezy7Qyu0wjc9uISwIAAOiaM6wAAHTt9f5wQFenX0+fPv36D9qi5eXlJjn79u1rknP8+PEmObOzs01yGpvErzbqanZb2rt3b5OcS5cuNck5duxYk5zFxS3/Bq3tZHYbWl1dbZKzf//+Jjnz8/NNclq9rsa2e3a7mtsTJ040yzp69GiTnNtvv71JztraWpOcaesLzrACANA1hRUAgK4prAAAdE1hBQCgaworAABdU1gBAOiawgoAQNcUVgAAuqawAgDQNYUVAICuKawAAHRNYQUAoGsKKwAAXVNYAQDomsIKAEDXFFYAALqmsAIA0LVdk17AG7G8vNws6/z5801yNjY2muTcfPPNTXIefvjhJjlJcuDAgWZZtDMzM9Mk57HHHmuS8+ijjzbJWVxcbJJDW+vr682y7rnnniY5N910U5OcCxcuNMmhnaNHjzbJafm58NSpU01yDh8+3CRnbW2tSc7CwkKTnO3iDCsAAF1TWAEA6JrCCgBA1xRWAAC6prACANA1hRUAgK4prAAAdE1hBQCgaworAABdU1gBAOiawgoAQNcUVgAAuqawAgDQNYUVAICuKawAAHRNYQUAoGsKKwAAXdu1HTtZW1trknP+/PkmOUny1FNPNcmZm5trkrNv374mOa2OdZIcOHCgWdZOt76+3ixrdXW1WVYL8/Pzk14CV9GZM2eaZd1xxx1Ncvbv398k59ixY01yaOfQoUNNcpaXl5vkJMmePXua5Nx+++1NchYWFprkTBtnWAEA6JrCCgBA1xRWAAC6prACANA1hRUAgK4prAAAdE1hBQCgaworAABdU1gBAOiawgoAQNcUVgAAuqawAgDQNYUVAICuKawAAHRNYQUAoGsKKwAAXVNYAQDomsIKAEDXdm3HTjY2Nprk3HnnnU1ykmRubq5ZVgt79uyZ9BK4gpMnTzbJWVlZaZKTJC+88EKzrBb27t076SVwFR05cqRZ1u7du5vktFrT4uJikxzaafW5+emnn26SkyTnz59vkrOwsNAkp1Wnmp2dbZKzXZxhBQCgaworAABdU1gBAOiawgoAQNcUVgAAuqawAgDQNYUVAICuKawAAHRNYQUAoGsKKwAAXVNYAQDomsIKAEDXFFYAALqmsAIA0DWFFQCArimsAAB0TWEFAKBru7ZjJxsbG01y9u3b1ySnR62O0ezsbJMcRo4cOdIkZ2lpqUlO0t/7+NKlS5NeAlfQ6v1y8uTJJjlJcubMmWZZLTz00EOTXgJXydzcXLOsixcvNslZWFjoKufs2bNNcpLt+bzkDCsAAF1TWAEA6JrCCgBA1xRWAAC6prACANA1hRUAgK4prAAAdE1hBQCgaworAABdU1gBAOiawgoAQNcUVgAAuqawAgDQNYUVAICuKawAAHRNYQUAoGsKKwAAXdu1HTuZnZ1tkrO2ttYkp6WNjY0mOefOnWuSc/DgwSY5sFXr6+tNcubn55vkMLKystIk5/7772+S09KZM2ea5MzMzDTJ4drWqsOcPXu2Sc7hw4eb5Jw4caJJTpIcP368WdZrcYYVAICuKawAAHRNYQUAoGsKKwAAXVNYAQDomsIKAEDXFFYAALqmsAIA0DWFFQCArimsAAB0TWEFAKBrCisAAF1TWAEA6JrCCgBA1xRWAAC6prACANA1hRUAgK7t2o6dzM3NNck5d+5ck5wkOX36dFc5rSwvL096CUAHlpaWmuSsrq42yUmSxx9/vEnO/v37m+QsLi42ybnvvvua5CTt1rTTHT16tFnWwsJCk5yNjY0mOY888kiTnIMHDzbJ2S7OsAIA0DWFFQCArimsAAB0TWEFAKBrCisAAF1TWAEA6JrCCgBA1xRWAAC6prACANA1hRUAgK4prAAAdE1hBQCgaworAABdU1gBAOiawgoAQNcUVgAAuqawAgDQNYUVAICu7dqOnczNzTXJOXHiRJOcJFleXm6Sc9dddzXJWVtba5JDn2ZmZpplLS4uNsn50pe+1CRndXW1Sc7S0lKTHEbm5+eb5KyvrzfJaZm1srLSJKfV/4Hdu3c3yUna/f/e6WZnZ5tlHTp0qFlWCwcPHmySc+rUqSY528UZVgAAuqawAgDQNYUVAICuKawAAHRNYQUAoGsKKwAAXVNYAQDomsIKAEDXFFYAALqmsAIA0DWFFQCArimsAAB0TWEFAKBrCisAAF1TWAEA6JrCCgBA1xRWAAC6VsMwTHoNAADwmpxhBQCgaworAABdU1gBAOiawgoAQNcUVgAAuqawAgDQtf8D+j89WnkesYsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Using the gini coefficient\n",
    "We talked about the gini index in the videos.  The gini coefficient for a **given split** is given by:\n",
    "$$Gini=\\sum_{t=1}^T \\frac{n_t}{N}gini(t)$$\n",
    "where $T$ is the total number of splits (2 for binary attributes), $n_t$ is the number of instances in node $t$ after splitting, and $N$ is the total number of instances in the parent node. $gini(t)$ is the **gini index for each individual node that is created by the split** and is given by:\n",
    "$$gini(t)=1-\\sum_{j=0}^{C-1} p(j|t)^2$$\n",
    "where $C$ is the total number of possible classes and $p(j|t)$ is the probability of class $j$ in node $t$ (i.e., $n_j==$ the count of instances belonging to class $j$ in node $t$, normalized by the total number of instances in node $t$).\n",
    "$$ p(j|t) = \\frac{n_j}{n_t}$$ \n",
    "\n",
    "For the given dataset, $gini(t)$ has been programmed for you in the function `gini_index`. \n",
    "\n",
    "* `def gini_index(classes_in_split):`\n",
    " * To use the function, pass in a `numpy` array of the class labels for a node as (i.e., pass in the rows from `ds.target` that make up a node in the tree) and the gini will be returned for that node. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# compute the gini of several examples for the starting dataset\n",
    "# This function \"gini_index\" is written for you. Once you run this block, you \n",
    "#   will have access to the function for the notebook. You do not need to know \n",
    "#   how this function works--only what it returns \n",
    "# This function returns the gini index for an array of classes in a node.\n",
    "def gini_index(classes_in_split):\n",
    "    # pay no attention to this code in the function-- it just computes the gini for a given split \n",
    "    classes_in_split = np.reshape(classes_in_split,(len(classes_in_split),-1))\n",
    "    unique_classes = np.unique(classes_in_split)\n",
    "    gini = 1\n",
    "    for c in unique_classes:\n",
    "        gini -= (np.sum(classes_in_split==c) / float(len(classes_in_split)))**2\n",
    "        \n",
    "    return gini"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the example below, the function is used calculate the gini for splitting the dataset on feature 28, with value 2.5. In this example, we need to create two separate tree nodes: the first node has all the `ds.target` labels when feature 28 is greater than 2.5, the second node has all the rows from `ds.target` where feature 28 is less than 2.5. The steps are outlined below. **Read this carefully to understand what the code does below in the block following this.**\n",
    "- Feature 28 is saved into a separate variable `feature28 = ds.data[:,28]`\n",
    "- First all the target classes for the first node are calculated using `numpy` indexing `ds.target[feature28>2.5]` \n",
    " - Note: this grabs all the rows in `ds.target` (the classes) which have feature 28 greater than 2.5 (similar to indexing in pandas)\n",
    "- Second, those classes are passed into the function to get the gini for the right node in this split (i.e., feature 28 being greater than the threshold 2.5). \n",
    " - `gini_r = gini_index(ds.target[feature28>2.5])`\n",
    "- Third, the gini is calculated for the left node in the tree. This grabs only the rows in `ds.target` where feature 28 is less than 2.5.\n",
    "     - `gini_l = gini_index(ds.target[feature28<=2.5])`\n",
    "- Combining the gini indices is left as an exercise in the next section"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#==========================Use the gini_index Example===============\n",
    "# get the value for this feature as a column vector \n",
    "# (this is like grabbing one column of the record table)\n",
    "feature28 = ds.data[:,28]\n",
    "\n",
    "# if we split on the value of 2.5, then this is the gini for each resulting node:\n",
    "gini_r = gini_index(ds.target[feature28>2.5]) # just like in pandas, we are sending in the rows where feature28>2.5\n",
    "gini_l = gini_index(ds.target[feature28<=2.5]) # and sending the rows where feature28<=2.5\n",
    "\n",
    "# compute gini example. This splits on attribute '28' with a value of 2.5\n",
    "print('gini for right node of split:', gini_r)\n",
    "print('gini for left node of split:', gini_l)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gini for right node of split: 0.8845857867667073\n",
      "gini for left node of split: 0.7115407566535388\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 2:** Now, using the above values `gini_r` and `gini_l`. Calculate the combined Gini for the entire split. You will need to write the weighted summation (based upon the number of instances inside each node). To count the number of instances greater than a value using numpy, you can use broadcasting, which is a special way of indexing into a numpy array. For example, the code `some_array>5` will return a new numpy array of true/false elements. It is the same size as `some_array` and is marked true where the array is greater than `5`, and false otherwise. By taking the `sum` of this array, we can count how many times `some_array` is greater than `5`. \n",
    "\n",
    "`counts = sum(some_array>5)` \n",
    " \n",
    "You will need to use this syntax to count the values in each node as a result of splitting.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "ds.data[:,28]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0., 16., 15., ..., 16.,  7., 16.])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "## Enter your code here\n",
    "feature28 = ds.data[:,28]\n",
    "\n",
    "# Find number of instance inside each node\n",
    "num_instance_r = sum(feature28>2.5)\n",
    "print('The number of instance greater than 2.5 is', num_instance_r)\n",
    "num_instance_l = sum(feature28<=2.5)\n",
    "print('The number of instance less than or equal to 2.5 is', num_instance_l)\n",
    "\n",
    "# Calculate the combined Gini\n",
    "combined_gini = (num_instance_r*gini_r + num_instance_l*gini_l) / len(ds.data)\n",
    "\n",
    "print('The total gini of the split for a threshold of 2.5 is:', combined_gini)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of instance greater than 2.5 is 1398\n",
      "The number of instance less than or equal to 2.5 is 399\n",
      "The total gini of the split for a threshold of 2.5 is: 0.8461634345045179\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "________\n",
    "# Prediction for ICA2 - PartB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**These contents will become available during the live session: **\n",
    "* <a href=\"#entropy\">Using Decision Trees - Entropy</a>\n",
    "* <a href=\"#multi\">Multi-way Splits</a>\n",
    "* <a href=\"#sklearn\">Decision Trees in Scikit-Learn</a>"
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entropy\n",
    "\n",
    "Prof. may provide us a function to calculate Entropy just like for Gini.\n",
    "Entropy value (using Feature28 at 2.5?).\n",
    "\n",
    "- Information Gain => Total_Entropy - sum of (n[i] / N * Entropy[i])\n",
    "- Split Information => sum of (n[i]/N log n[i]/N)\n",
    "- Gain Ratio => Information Gain / Split Information => "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Enter your code here\n",
    "feature28 = ds.data[:,28]\n",
    "\n",
    "# If we split on the value of 2.5, then this is the entropy for each resulting node:\n",
    "ent_r = **ENT_index**(ds.target[feature28>2.5]) \n",
    "ent_l = **ENT_index**(ds.target[feature28<=2.5])\n",
    "\n",
    "# Compute entropy\n",
    "print('entropy for right node of split:', gini_r)\n",
    "print('entropy for left node of split:', gini_l)\n",
    "\n",
    "# Find number of instance inside each node\n",
    "ent_num_instance_r = sum(feature28>2.5)\n",
    "print('The number of instance greater than 2.5 is', ent_num_instance_r)\n",
    "ent_num_instance_l = sum(feature28<=2.5)\n",
    "print('The number of instance less than or equal to 2.5 is', ent_num_instance_l)\n",
    "\n",
    "# Calculate the information gain\n",
    "information_gain = TOTAL_ENTROPY - ((ent_num_instance_r*ent_r + ent_num_instance_l*ent_l) / len(ds.data))\n",
    "\n",
    "print('The information gain of the split for a threshold of 2.5 is:', information_gain)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple-Way Splits\n",
    "\n",
    "- Gini total - Weighter Partitions?\n",
    "- Information Gain from Entropy?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Calculate the combined Gini\n",
    "combined_gini = (num_instance_a*gini_r + num_instance_l*gini_l) / len(ds.data)\n",
    "\n",
    "# Calculate the information gain\n",
    "information_gain = TOTAL_ENTROPY - ((ent_num_instance_r*ent_r + ent_num_instance_l*ent_l) / len(ds.data))\n",
    "\n",
    "# Split Information can be used to normalize Information Gain => Gain Raio"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'num_instance_a' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vz/68jlc85d78vcqcr5gx1r56l40000gn/T/ipykernel_31736/2200426498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate the combined Gini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcombined_gini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_instance_a\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgini_r\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_instance_l\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgini_l\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Calculate the information gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minformation_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOTAL_ENTROPY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment_num_instance_r\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ment_r\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ment_num_instance_l\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ment_l\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_instance_a' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Trees in Scikit-Learn\n",
    "\n",
    "<a>https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>\n",
    "\n",
    "<a>https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart</a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# now get the training and testing\n",
    "for train, test in cv.split(X,y):\n",
    "    print ('Training Once:')\n",
    "    # train the decision tree algorithm\n",
    "    %time dt_clf.fit(X[train],y[train]) ##############\n",
    "    yhat = dt_clf.predict(X[test]) ############\n",
    "    print ('accuracy:', mt.accuracy_score(y[test],yhat))\n",
    "\n",
    "\n",
    "\n",
    "yhat = dt_clf.predict(X[test])\n",
    "class_probabilities = dt_clf.predict_proba(X[test])\n",
    "print ('Accuracy:', mt.accuracy_score(y[test],yhat))\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=ds.target, y=y_hat)\n",
    "plt.xlabel(\"y_actual\")\n",
    "plt.ylabel(\"y_hat\")\n",
    "plt.title(\"Actual vs. Prediction\") "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('ML7331': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "interpreter": {
   "hash": "cbda91dd9203448bc32b4be90dc652a7612a0a1c1eadb39ecd15f24cd65e5247"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}